Adam Moody <moody20@llnl.gov>
Lawrence Livermore National Laboratory

====================================================
PMGR_COLLECTIVE Overview
====================================================
PMGR_COLLECTIVE was motivated by two design goals:
  1) To provide flexibility in the MPI bootstapping.
  2) To provide scalable startup.

This library enables MPI to bootstrap itself through a series of collective
operations.  The collective operations are modeled after MPI collectives --
all tasks must call them in the same order and with consistent parameters.
MPI may invoke any number of collectives, in any order, passing an arbitrary
amount of data.  All message sizes are specified in bytes.

The library consists of two interfaces, one which is called by the job
launcher (e.g., mpirun) and one which is called by the MPI tasks:

  pmgr_collective_mpirun - called by mpirun
  pmgr_collective_client - called by the MPI tasks

pmgr_collective_mpirun - called by mpirun
  The mpirun process should call pmgr_processops after accepting connections
  from the MPI tasks and negotiating the protocol version number (PMGR_COLLECTIVE
  uses protocol 8).

  It should provide an array of open socket file descriptors indexed by MPI rank
  (fds) along with the number of MPI tasks (nprocs) as arguments.

  pmgr_processops will handle all PMGR_COLLECTIVE operations and return control
  upon an error or after receiving PMGR_CLOSE from the MPI tasks.  If no errors
  are encountered, it will close all socket file descriptors before returning.
  It returns PMGR_SUCCESS on successful completion.

pmgr_collective_client - called by the MPI tasks
  An MPI task should make calls in the following sequenece:
    pmgr_init
    pmgr_open
    [pmgr_collectives]
    pmgr_close
    pmgr_finalize
  All functions return PMGR_SUCCESS on successful completion.

====================================================
Environment Variables for mpirun
====================================================
All environment variables are read when mpirun calls pmgr_processops().

Debug statements can be turned on for troubleshooting or timing:

  MPIRUN_DEBUG={0,1,2}
    0 - disable debug statements
    1 - first debug verbosity, includes timing
    2 - increased debug verbosity

====================================================
Environment Variables for the Client
====================================================
All environment variables are read when a client calls pmgr_init().

The following variables must be set to define the client's MPI context:

  MPIRUN_NPROCS - number of MPI processes in job       must be >0
  MPIRUN_RANK   - the client's MPI rank in the job     must be in [0,N-1]
  MPIRUN_ID     - unique jobid of current application  must be !=0
  MPIRUN_HOST   - IP address of the mpirun process in dotted decimal notation
  MPIRUN_PORT   - port number of the mpirun process    must be >0

Debug statements can be turned on for troubleshooting or timing.  For each
level of debug verbosity, there are two sublevels.  In the first sublevel,
just rank 0 and rank N-1 print information.  All ranks print debug statements
in the second sublevel.

  MPIRUN_CLIENT_DEBUG={0-6}
    0                     - disable debug statements

    ranks[0,N]  all ranks
    1           2         - includes pmgr_open to pmgr_close timing
    3           4         - includes timing for each pmgr_* function
    5           6         - denotes start and exit for each pmgr_* function

====================================================
Current Implementation
====================================================
The current implementation sets up a binomial tree via TCP sockets with rank 0 at the root.
During pmgr_open(), each client process opens a TCP socket to accept connections and passes
its IP:port to mpirun.  After gathering the IP:port entry for each process, mpirun passes
this socket table to rank 0.  Rank 0 then connects to each of its children (now that it has
everyone's address) and forwards the table to each.  Each child receives the socket table,
opens a connection with any children it has, and forwards the table.  The process continues
down the tree until a leaf child is reached which just accepts the incoming parent connection.

This same binomial tree, which was used to distribute the socket table, is also used to gather
and broadcast data during collective operations.  Namely, this tree is used to provide log(N)
scalable algorithms for:

  pmgr_barrier
  pmgr_allgather
  pmgr_bcast  (when rank 0 is the root)
  pmgr_gather (when rank 0 is the root)

The remaining collectives are implemented via mpirun support, which typically amounts to a
flat tree with mpirun as the root.  These collectives thus scale linearly:

  pmgr_bcast  (when rank 0 is not the root)
  pmgr_gather (when rank 0 is not the root)
  pmgr_scatter
  pmgr_alltoall

This implementation works best if the TCP socket each task opens corresponds to a port
on a switched network so that multiple nodes may send and receive data at the same time.

It's possible to disable / enable the binomial tree at runtime:

  MPIRUN_USE_TREES={0,1}       - disable/enable all trees (disabling also avoids setup and teardown)
  MPIRUN_USE_GATHER_TREE={0,1} - disable/enable gather tree
  MPIRUN_USE_BCAST_TREE={0,1}  - disable/enable bcast tree

If a tree is disabled, then the collective will fallback to the mpirun
support, which is typically a flat tree with the mpirun process at the root.

By default, the TREE environment variables are all enabled.  The library can be
built with these variables disabled by default.  To do this, define the appropriate
macros on the compile line.  For example, to disable all by default, you'd include
all of the following in your compile flags:

  -DMPIRUN_USE_TREES=0
  -DMPIRUN_USE_GATHER_TREE=0
  -DMPIRUN_USE_BCAST_TREE=0

====================================================
Ideas for Future Work (if the demand arises)
====================================================
It's possible to extend this implementation to set set up other TCP topograhies (in addition
to the binomial tree) via the socket table to provide scalable support for other collectives.

A scalable PMI could probably be implemented on top of PMGR_COLLECTIVE using pmgr_allgather.

Reliability could be added to retry or reestablish connections.

As process counts increase higher and higher, at some point, mpirun itself will likely need
to collect the IP:port entries via a tree network to build the socket table.  This could be
done transparently from PMGR_COLLECTIVES point of view.
